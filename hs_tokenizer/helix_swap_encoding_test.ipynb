{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode with HS Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helix_swap_bpe as hsb\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_path = '/scratch/jbusch/ma/tokenizer/hs_tokenizer/fixed_encode_decode/sub5M_tokenizer.json'\n",
    "input_file = '/scratch/jbusch/ma/data/dnabert_2_pretrain/dev.txt'\n",
    "output_file = '/scratch/jbusch/ma/data/dnabert_2_pretrain/hs_tokenized/sub5M_dev.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tokenizer\n",
    "tokenizer = hsb.helixswap_loader(tokenizer_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [\"AATTATATATATATATATATGGCCCACACggacaaaaaa\",\"AGGAGTATAVGGACCAGATTGCAC\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = tokenizer.encode(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AATTATATATATATATATATGGCCCACACggacaaaaaa\n",
      "AGGAGTATAVGGACCAGATTGCAC\n",
      "Examples of decoded sequences\n",
      "AATTATATATATATATATATGGCCCACACGGACAAAAAA\n",
      "AGGAGTATA[UNK]GGACCAGATTGCAC\n"
     ]
    }
   ],
   "source": [
    "print(sequences[0])\n",
    "print(sequences[1])\n",
    "\n",
    "print('Examples of decoded sequences')\n",
    "print(tokenizer.decode(tokenized[0]))\n",
    "print(tokenizer.decode(tokenized[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[0].upper() == tokenizer.decode(tokenized[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name = 'input_ids'\n",
    "dev = pd.read_csv(input_file, sep=\" \", names=[column_name])\n",
    "test = dev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CTGTGAACAAGCTTCTCTCGTGCACTCATGAACGCGCAACAGATTTTCACTACAAAATGACTTCGCCCAAAATTTAAAATGTTTTCTTAATTTACTCAACCTCATGCTATCCCAGATCTTTGACTATTTTCATCTGCTAAACACAAACGAAATTTAGAAAAATATCTCAGCTCTGGTGAATGGTGATCAGAACTTTGAAGGTCCAGAAAGCACAAAGGCAGCATAATAGTAATCCACATGACTCCAGTGGTTAAATCCATATCTTCAGAAGCAATATGGGTGAGAAAACGATCCCAATTCTCCTCCCTGCCCAGCAGGTGTCGATATGCACAAAGAATGTGAATTGCCAAAAACAAAAGAAGATTTCTAGTAAAAAAAGGACTTAAATATGGATCTGTGTCTTACCCACACCTATCATACCACTTCTGAATATATAGACTTTACCACTGGTGTCTTATGGATTACTTTTATGCTGCCTTTATATGCTTTTTGGACCTTAAAAGTTCTGGCCACCATTCACTTGCATTGTACAGAGCTGAGATATTTTAAAAAAAAATCTTTGTGTTCAGCAGAAGAAAGACATTTATACACATCTGGGATGGCATGAGGGTGTTTTAACGAAGAGAATTTTAAAATTGTATGCCTTCAGAAGACCTGGAAAATGACACTGGAGTCGCATGAGGGCGAGTAAAGTAGGACAGAATTTCCATTTTTTGGATGAAATATTCCTTTGATAAAAAGTATTTTCTCAAAGTTATGCCTTATGACTTATCTCAAAGTGTAACTACTTTGTAACAATTGTAACAATATGATCAAACACAGCCTTAAGTGTTGTCTTGACATATTATCACTTAAGTTTTTGACCCTTTTTATTATAATTCAACTTACATTTTTTAAATAGCGTCTCTGAAACACTGTGGTACCCAGCAGACCAGATTTCTAGGGTTTTGAGTGGTGGCTTGCTAGCCCAAGTCAAAAGAGCCCATCCTCAGGTCTCTAC'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_list = test.input_ids.to_list()\n",
    "input_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3376,\n",
       " 1385,\n",
       " 84,\n",
       " 57,\n",
       " 1423,\n",
       " 859,\n",
       " 49,\n",
       " 1096,\n",
       " 98,\n",
       " 846,\n",
       " 132,\n",
       " 512,\n",
       " 178,\n",
       " 3520,\n",
       " 34,\n",
       " 170,\n",
       " 50,\n",
       " 2901,\n",
       " 1777,\n",
       " 80,\n",
       " 1654,\n",
       " 122,\n",
       " 509,\n",
       " 42,\n",
       " 644,\n",
       " 120,\n",
       " 342,\n",
       " 72,\n",
       " 125,\n",
       " 524,\n",
       " 198,\n",
       " 18,\n",
       " 731,\n",
       " 3419,\n",
       " 224,\n",
       " 326,\n",
       " 168,\n",
       " 52,\n",
       " 584,\n",
       " 294,\n",
       " 164,\n",
       " 1021,\n",
       " 275,\n",
       " 164,\n",
       " 167,\n",
       " 34,\n",
       " 1225,\n",
       " 208,\n",
       " 64,\n",
       " 785,\n",
       " 71,\n",
       " 55,\n",
       " 661,\n",
       " 43,\n",
       " 340,\n",
       " 183,\n",
       " 609,\n",
       " 1365,\n",
       " 269,\n",
       " 24,\n",
       " 136,\n",
       " 982,\n",
       " 49,\n",
       " 438,\n",
       " 109,\n",
       " 848,\n",
       " 1479,\n",
       " 452,\n",
       " 100,\n",
       " 158,\n",
       " 606,\n",
       " 190,\n",
       " 2520,\n",
       " 949,\n",
       " 391,\n",
       " 50,\n",
       " 512,\n",
       " 30,\n",
       " 304,\n",
       " 192,\n",
       " 1234,\n",
       " 240,\n",
       " 40,\n",
       " 150,\n",
       " 46,\n",
       " 438,\n",
       " 1233,\n",
       " 128,\n",
       " 42,\n",
       " 53,\n",
       " 260,\n",
       " 491,\n",
       " 53,\n",
       " 165,\n",
       " 3276,\n",
       " 67,\n",
       " 893,\n",
       " 184,\n",
       " 52,\n",
       " 980,\n",
       " 182,\n",
       " 2285,\n",
       " 209,\n",
       " 1224,\n",
       " 160,\n",
       " 46,\n",
       " 1751,\n",
       " 274,\n",
       " 86,\n",
       " 41,\n",
       " 165,\n",
       " 28,\n",
       " 296,\n",
       " 1000,\n",
       " 2344,\n",
       " 105,\n",
       " 169,\n",
       " 327,\n",
       " 540,\n",
       " 280,\n",
       " 757,\n",
       " 278,\n",
       " 99,\n",
       " 3222,\n",
       " 315,\n",
       " 1031,\n",
       " 628,\n",
       " 126,\n",
       " 521,\n",
       " 397,\n",
       " 90,\n",
       " 139,\n",
       " 51,\n",
       " 49,\n",
       " 788,\n",
       " 578,\n",
       " 50,\n",
       " 2695,\n",
       " 1122,\n",
       " 597,\n",
       " 1753,\n",
       " 3493,\n",
       " 660,\n",
       " 1932,\n",
       " 90,\n",
       " 2497,\n",
       " 36,\n",
       " 460,\n",
       " 159,\n",
       " 795,\n",
       " 3232,\n",
       " 409,\n",
       " 73,\n",
       " 119,\n",
       " 1318,\n",
       " 360,\n",
       " 68,\n",
       " 1570,\n",
       " 214,\n",
       " 872,\n",
       " 174,\n",
       " 4064,\n",
       " 160,\n",
       " 214,\n",
       " 133,\n",
       " 36,\n",
       " 2670,\n",
       " 3141,\n",
       " 112,\n",
       " 226,\n",
       " 24,\n",
       " 589,\n",
       " 203,\n",
       " 33,\n",
       " 131,\n",
       " 323,\n",
       " 363,\n",
       " 120,\n",
       " 354,\n",
       " 160,\n",
       " 322,\n",
       " 176,\n",
       " 51,\n",
       " 274,\n",
       " 69,\n",
       " 2204,\n",
       " 432,\n",
       " 49,\n",
       " 1920,\n",
       " 3674,\n",
       " 77,\n",
       " 2502,\n",
       " 3428,\n",
       " 210,\n",
       " 2363,\n",
       " 3745,\n",
       " 304,\n",
       " 192,\n",
       " 187,\n",
       " 215,\n",
       " 297,\n",
       " 1390,\n",
       " 192,\n",
       " 336,\n",
       " 92,\n",
       " 384,\n",
       " 242,\n",
       " 126,\n",
       " 326,\n",
       " 19,\n",
       " 1129]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized = tokenizer.encode(input_list)\n",
    "tokenized[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(input_list[0] == tokenizer.decode(tokenized[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenized_sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[626, 97, 363, 1522, 899, 130, 110, 74, 136, 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[3376, 1385, 84, 57, 1423, 859, 49, 1096, 98, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[269, 67, 185, 1081, 92, 34, 797, 302, 6, 173,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[3844, 1684, 1084, 95, 95, 111, 1518, 964, 144...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[8, 197, 41, 126, 50, 2377, 3999, 166, 228, 60...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  tokenized_sequence\n",
       "0  [626, 97, 363, 1522, 899, 130, 110, 74, 136, 4...\n",
       "1  [3376, 1385, 84, 57, 1423, 859, 49, 1096, 98, ...\n",
       "2  [269, 67, 185, 1081, 92, 34, 797, 302, 6, 173,...\n",
       "3  [3844, 1684, 1084, 95, 95, 111, 1518, 964, 144...\n",
       "4  [8, 197, 41, 126, 50, 2377, 3999, 166, 228, 60..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'tokenized_sequence': tokenized})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('test_tokenized_sequences.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helix Swap as HF Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cluster_home/jbusch/conda/envs/bens_tokenizer_39_less_output/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Tokenizer' from 'transformers' (/home/cluster_home/jbusch/conda/envs/bens_tokenizer_39_less_output/lib/python3.9/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tokenizer\n\u001b[1;32m      3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m Tokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(tokenizer_path)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Tokenizer' from 'transformers' (/home/cluster_home/jbusch/conda/envs/bens_tokenizer_39_less_output/lib/python3.9/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "from transformers import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer.from_pretrained(tokenizer_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bens_tokenizer_39_less_output",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
